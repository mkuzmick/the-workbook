---
title: some text for reports
tags: [cd]

---

---
tags: cd
---
# some text for reports

## workshops to write 3 sentences about
* emr 162: race and ai
* eng 189vg
* slavic 121
* emr 163: graphic novels
* complit 280x
* social studies 10 (Foucault)
* complit 200


## some sentences

### AI framing text
This year at the Learning Lab, we've focused intensely on AI, recognizing it as a crucial emergent phenomenon in the realms of teaching, learning, and knowledge production. Our approach centers on understanding how students might use AI—both text and image generation AI tools—in the classroom for a positive and proactive learning experience. Students at many Learning Lab workshops experimented with AI tools for transcription, image recognition, and production, enabling them to achieve greater creativity and technical sophistication as they seamlessly traversed the boundary between traditional and digital media.

### emr 162: race and AI

[This start is too clunky, but:] The dimension of this course for which we provided support focused on questions of potential intrinsic bias embedded in AI's generative prediction architecture. We introduced the students to different models of image generation, toggling between GPT-4, Gemini Advanced, and Midjourney and exploring their aesthetic forms and limitations. We led this workshop at a time when it had become clear to the public that these models nearly always and consistently took as a starting point [this needs to be phrased more accurately] culturally normative frameworks for their generation of images of humans. Almost immediately, as the models sought to fix this, their attempts resulted in a kind of over-correction. Specifically, in the effort to generate images of groups of people that were more ethnically diverse, the models would create images that were culturally and historically impossible. The backlash from this was sufficiently strong that Google completely disabled Gemini's capacity to create images of people. OpenAI continued to permit their predictive generation and built within it a structure of revised prompts that would proactively diversify any group. We wrote a ____ that returned GPT's JSON file(?), which enabled us to view the revised prompt normally hidden from the user interface. The revised promts are generally long, quite elaborate and atmospheric, and inevitably diversify the racial makeup of any human group. With the students, we explored and analyzed the content, structure, and rationale of these hidden texts. We also built a Python Notebook that would take the revised prompt and further manipulate it, generating images from its own formualtions, with an extra step of asking it to create the model's interpretation of what the *opposite* of that image would be. We then set the code to generate opposing images recursively, to see where a dialectical method would lead and what implications that would have for the depiction of race. We explored how the models would depict non-Western *places* as well, as the course was also invested in questions of stereotyped representation of broader cultural context, including aesthetics, landscape, and architecture. Students came with pre-prepared prompts and, after analysing those, we ran them through the engines we built, both to unpack the results and their implications and to provide students with images and revised prompts that they could take with them for their final projects.                    


### eng 189vg stable diffusion and blender
The Learning Lab hosted a five-hour workshop for 167 students in ENGLISH 189VG Video Game Storytelling, structuring activities around game designer Jesse Schell's tetrad: aesthetics, mechanics, story, and technology. Across the stations, students learned how AI can be leveraged in the game design and production process. At the aesthetics station, Media & Design Fellow Chris Benham led students through an activity where they built basic environments out of 3D shapes in Blender, and then used Stable Diffusion to give detail, texture, and color to those 3D assets. By bringing their 3D assets into Stable Diffusion--an image generation AI tool--students instantaneously created 3D game worlds that they designed themselves. Students also used the image generation tool Midjourney to create custom characters at a story station where they learned how to reveal the game's plot through character development and dialogue. Through this workshop, students learned how to use AI to simplify what were previously highly complex production processes and to learn a bit about the types of parameters to put in place when using image generation tools like Stable Diffusion.

### Slavic 121/tdm 121k ballet stable diffusion
Students in SLAVIC 121/TDM 121K: Ballet, Past and Present came to the Learning Lab for a workshop where they engaged in a number of different multimodal activities to reflect on ballet—specifically, the way knowledge about ballet is transmitted through time and how media can be used to annotate, analyze, and even reproduce the ballet movements. This workshop used media to address longstanding interests among practitioners and scholars of ballet with annotating the movement of the body and isolating parts of the body for close analysis. At one station, students worked with printed frame-by-frame stills from the ballets they were analyzing, annotating them as they would in a visual essay. Using paper to perform on-the-spot analysis of ballet both strengthened students’ ability to analyze the form, while it also helped them consider the types of visuals they would integrate into a visual essay. At another station, students learned the basic video editing skills required to isolate specific ballet movements, juxtaposing movements in a split-screen style or assembling a series of movements as a montage, or exporting a series of looping gifs that could be embedded into a presentation or essay. Finally, students posed on our green screen stage, then used computer vision tools to analyze and isolate the positions of their bodies. These bodily positions were then fed into the image generation tool Stable Diffusion, the main open-source AI image generation tool. Students learned to use the images of their own poses as “controllers” that could strictly determine key features of the exported image. In a sense, their bodies became the "input devices" that controlled the AI generation in thought provoking ways.


### complit280x
In COMPLIT280X: Data and Transmedia Storytelling, students coded interactions with GPT to produce AI-generated poems. Having created this backend interaction, students came to the Learning Lab to learn about the range of web development tools they could use to style their poetry (creating a “front end” view). Media & Design Fellow Anna Ivanov designed an example website that displayed side-by-side two translations of a poem using the React framework Next.js, which is an industry-standard tool used to create interactive websites. Students also learned some HTML and CSS basics, which could be used to style static views of their poems. Through this workshop, students learned how using code for styling enables a basically infinite number of extremely precise aesthetic possibilities. And in having to specify in the code what aesthetics they wanted for their poems, students thought the way that poets do about margin widths, font pairings, colors, line breaks, and other graphical considerations.